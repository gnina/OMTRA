{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "722f4e95-99d5-480e-a6c3-b742c0ef19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numpy as np\n",
    "import math, random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as torch_data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numcodecs  # for Pickle or JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13d63ba-f307-413d-9f74-76078f766fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67dfb39e-5cf1-455b-804a-23c8cca1d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "n_molecules = 100\n",
    "\n",
    "unbatched_molecules = defaultdict(list)\n",
    "for _ in range(n_molecules):\n",
    "    n_atoms = np.random.randint(5, 15)\n",
    "    n_edges = np.random.randint(n_atoms // 2, n_atoms * 2)\n",
    "    \n",
    "    x = np.random.randn(n_atoms, 3)              # positions\n",
    "    a = np.random.randint(0, 5, size=n_atoms)    # atom types\n",
    "    edge_idxs = np.random.randint(0, n_atoms, size=(n_edges, 2))\n",
    "    e = np.random.randint(0, 3, size=n_edges)    # bond orders\n",
    "\n",
    "    unbatched_molecules['x'].append(x)\n",
    "    unbatched_molecules['a'].append(a)\n",
    "    unbatched_molecules['edge_index'].append(edge_idxs)\n",
    "    unbatched_molecules['e'].append(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73b5cdc8-2416-4690-a260-9137402f09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of graphs\n",
    "n_graphs = len(unbatched_molecules['x'])\n",
    "\n",
    "# Count the number of nodes/edges per graph\n",
    "batch_num_nodes = [arr.shape[0] for arr in unbatched_molecules['x']]\n",
    "batch_num_edges = [arr.shape[0] for arr in unbatched_molecules['edge_index']]\n",
    "\n",
    "batch_num_nodes = np.array(batch_num_nodes, dtype=np.int64)\n",
    "batch_num_edges = np.array(batch_num_edges, dtype=np.int64)\n",
    "\n",
    "# Concatenate all node data\n",
    "x = np.concatenate(unbatched_molecules['x'], axis=0)  # shape (total_nodes, 3)\n",
    "a = np.concatenate(unbatched_molecules['a'], axis=0)  # shape (total_nodes,)\n",
    "\n",
    "# Concatenate all edge data\n",
    "edge_index = np.concatenate(unbatched_molecules['edge_index'], axis=0)  # shape (total_edges, 2)\n",
    "e = np.concatenate(unbatched_molecules['e'], axis=0)                    # shape (total_edges,)\n",
    "\n",
    "# Build lookup for each graph's node range\n",
    "node_lookup = np.zeros((n_graphs, 2), dtype=np.int64)\n",
    "node_lookup[1:, 0] = np.cumsum(batch_num_nodes[:-1])\n",
    "node_lookup[:, 1] = np.cumsum(batch_num_nodes)\n",
    "\n",
    "# Build lookup for each graph's edge range\n",
    "edge_lookup = np.zeros((n_graphs, 2), dtype=np.int64)\n",
    "edge_lookup[1:, 0] = np.cumsum(batch_num_edges[:-1])\n",
    "edge_lookup[:, 1] = np.cumsum(batch_num_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c5aaa1e-328b-4e1e-8955-2047a579303b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1153677/807597110.py:18: DeprecationWarning: Use Group.create_array instead.\n",
      "  ds_x = node_data_group.create_dataset(\n",
      "/tmp/ipykernel_1153677/807597110.py:24: DeprecationWarning: Use Group.create_array instead.\n",
      "  ds_a = node_data_group.create_dataset(\n",
      "/tmp/ipykernel_1153677/807597110.py:30: DeprecationWarning: Use Group.create_array instead.\n",
      "  ds_node_lookup = node_data_group.create_dataset(\n",
      "/tmp/ipykernel_1153677/807597110.py:38: DeprecationWarning: Use Group.create_array instead.\n",
      "  ds_edge_index = edge_data_group.create_dataset(\n",
      "/tmp/ipykernel_1153677/807597110.py:44: DeprecationWarning: Use Group.create_array instead.\n",
      "  ds_e = edge_data_group.create_dataset(\n",
      "/tmp/ipykernel_1153677/807597110.py:50: DeprecationWarning: Use Group.create_array instead.\n",
      "  ds_edge_lookup = edge_data_group.create_dataset(\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "store = zarr.storage.MemoryStore()\n",
    "root = zarr.group(store=store)\n",
    "\n",
    "node_data_group = root.create_group('node_data')\n",
    "edge_data_group = root.create_group('edge_data')\n",
    "\n",
    "# Let's pick chunk sizes based on ~10 graphs worth of nodes/edges\n",
    "graphs_per_chunk = 10\n",
    "mean_nodes_per_graph = int(batch_num_nodes.mean())\n",
    "mean_edges_per_graph = int(batch_num_edges.mean())\n",
    "\n",
    "nodes_per_chunk = graphs_per_chunk * mean_nodes_per_graph\n",
    "edges_per_chunk = graphs_per_chunk * mean_edges_per_graph\n",
    "\n",
    "# Create node-level arrays (empty initially)\n",
    "ds_x = node_data_group.create_dataset(\n",
    "    'x',\n",
    "    shape=x.shape,                # e.g. (total_nodes, 3)\n",
    "    dtype=x.dtype,\n",
    "    chunks=(nodes_per_chunk, x.shape[1])  # e.g. (some_number, 3)\n",
    ")\n",
    "ds_a = node_data_group.create_dataset(\n",
    "    'a',\n",
    "    shape=a.shape,                # e.g. (total_nodes,)\n",
    "    dtype=a.dtype,\n",
    "    chunks=(nodes_per_chunk,)\n",
    ")\n",
    "ds_node_lookup = node_data_group.create_dataset(\n",
    "    'node_lookup',\n",
    "    shape=node_lookup.shape,      # e.g. (n_graphs, 2)\n",
    "    dtype=node_lookup.dtype,\n",
    "    chunks=node_lookup.shape      # small enough, can store in 1 chunk\n",
    ")\n",
    "\n",
    "# Create edge-level arrays (empty initially)\n",
    "ds_edge_index = edge_data_group.create_dataset(\n",
    "    'edge_index',\n",
    "    shape=edge_index.shape,       # e.g. (total_edges, 2)\n",
    "    dtype=edge_index.dtype,\n",
    "    chunks=(edges_per_chunk, edge_index.shape[1])\n",
    ")\n",
    "ds_e = edge_data_group.create_dataset(\n",
    "    'e',\n",
    "    shape=e.shape,                # e.g. (total_edges,)\n",
    "    dtype=e.dtype,\n",
    "    chunks=(edges_per_chunk,)\n",
    ")\n",
    "ds_edge_lookup = edge_data_group.create_dataset(\n",
    "    'edge_lookup',\n",
    "    shape=edge_lookup.shape,      # e.g. (n_graphs, 2)\n",
    "    dtype=edge_lookup.dtype,\n",
    "    chunks=edge_lookup.shape\n",
    ")\n",
    "\n",
    "# Now write the data to each dataset\n",
    "ds_x[...] = x\n",
    "ds_a[...] = a\n",
    "ds_node_lookup[...] = node_lookup\n",
    "\n",
    "ds_edge_index[...] = edge_index\n",
    "ds_e[...] = e\n",
    "ds_edge_lookup[...] = edge_lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10352795-bc38-428a-8008-d849cf28882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "class MyStorage:\n",
    "    \"\"\"\n",
    "    Single storage class for chunked numeric arrays (x, a, e, edge_index).\n",
    "    Provides chunk-level caching, so repeated reads in the same chunk\n",
    "    do not cause repeated disk I/O.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_group):\n",
    "        node_grp = root_group['node_data']\n",
    "        edge_grp = root_group['edge_data']\n",
    "        self.zarr_arrays = {\n",
    "            'x': node_grp['x'],\n",
    "            'a': node_grp['a'],\n",
    "            'e': edge_grp['e'],\n",
    "            'edge_index': edge_grp['edge_index'],\n",
    "        }\n",
    "\n",
    "        # store chunk sizes\n",
    "        self.chunk_sizes = {\n",
    "            name: arr.chunks[0] for name, arr in self.zarr_arrays.items()\n",
    "        }\n",
    "\n",
    "    def load_item(self, array_name, i):\n",
    "        \"\"\"\n",
    "        Return the single row i of array array_name (axis=0).\n",
    "        \"\"\"\n",
    "        chunk_size = self.chunk_sizes[array_name]\n",
    "        chunk_num, chunk_idx = divmod(i, chunk_size)\n",
    "        chunk_data = self._load_chunk(array_name, chunk_num)\n",
    "        return chunk_data[chunk_idx]\n",
    "\n",
    "    @functools.lru_cache(None)\n",
    "    def _load_chunk(self, array_name, chunk_num):\n",
    "        arr = self.zarr_arrays[array_name]\n",
    "        chunk_size = self.chunk_sizes[array_name]\n",
    "        start = chunk_num * chunk_size\n",
    "        end = start + chunk_size\n",
    "        return arr[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "809d1240-2f82-4076-8ec9-5aaa9934b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ZarrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns (x, a, e, edge_index) for graph `idx`.\n",
    "    Uses a single MyStorage for chunk-cached reads.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_group):\n",
    "        node_grp = root_group['node_data']\n",
    "        edge_grp = root_group['edge_data']\n",
    "\n",
    "        # lookups are small, read into memory\n",
    "        self.node_lookup = node_grp['node_lookup'][:]\n",
    "        self.edge_lookup = edge_grp['edge_lookup'][:]\n",
    "        self.n_graphs = self.node_lookup.shape[0]\n",
    "\n",
    "        self.storage = MyStorage(root_group)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_graphs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Node range\n",
    "        node_start, node_end = self.node_lookup[idx]\n",
    "        # Edge range\n",
    "        edge_start, edge_end = self.edge_lookup[idx]\n",
    "\n",
    "        # read node-level rows\n",
    "        x_list = [self.storage.load_item('x', i) for i in range(node_start, node_end)]\n",
    "        a_list = [self.storage.load_item('a', i) for i in range(node_start, node_end)]\n",
    "\n",
    "        # read edge-level rows\n",
    "        e_list = [self.storage.load_item('e', j) for j in range(edge_start, edge_end)]\n",
    "        edge_idx_list = [self.storage.load_item('edge_index', j) for j in range(edge_start, edge_end)]\n",
    "\n",
    "        # convert to arrays or torch tensors\n",
    "        x = np.vstack(x_list).astype(np.float32)      # shape (num_nodes, 3)\n",
    "        a = np.array(a_list, dtype=np.int64)          # shape (num_nodes,)\n",
    "        e = np.array(e_list, dtype=np.int64)          # shape (num_edges,)\n",
    "        edge_index = np.vstack(edge_idx_list).astype(np.int64)  # shape (num_edges, 2)\n",
    "\n",
    "        return (x, a, e, edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0d50e09-18e4-47dd-9a29-254eb8c3c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math, random\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class TwoLevelSampler(Sampler[list[int]]):\n",
    "    \"\"\"\n",
    "    Sampler that:\n",
    "      1) Splits the dataset's [0..n_graphs-1] indices into chunks of size `chunk_size`.\n",
    "      2) Randomly shuffles the chunk order (outer random).\n",
    "      3) For each chunk, optionally shuffle the item order inside it (inner random).\n",
    "      4) Yields *mini-batches* of size `mini_batch_size` from that chunk.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: Dataset,\n",
    "        chunk_size: int,\n",
    "        mini_batch_size: int,\n",
    "        shuffle_chunks: bool = True,\n",
    "        shuffle_within_chunk: bool = True\n",
    "    ):\n",
    "        super().__init__(data_source)\n",
    "        self.n_graphs = len(data_source)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.shuffle_chunks = shuffle_chunks\n",
    "        self.shuffle_within_chunk = shuffle_within_chunk\n",
    "\n",
    "        self.num_chunks = math.ceil(self.n_graphs / self.chunk_size)\n",
    "\n",
    "        # Pre-build a list of chunks, each chunk is a list of graph indices\n",
    "        self.chunks = []\n",
    "        start = 0\n",
    "        for _ in range(self.num_chunks):\n",
    "            end = min(start + self.chunk_size, self.n_graphs)\n",
    "            self.chunks.append(list(range(start, end)))\n",
    "            start = end\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle the chunk order if requested\n",
    "        chunk_indices = list(range(self.num_chunks))\n",
    "        if self.shuffle_chunks:\n",
    "            random.shuffle(chunk_indices)\n",
    "\n",
    "        # For each chunk in random order\n",
    "        for cidx in chunk_indices:\n",
    "            indices_in_chunk = self.chunks[cidx]\n",
    "\n",
    "            # Shuffle inside chunk if requested\n",
    "            if self.shuffle_within_chunk:\n",
    "                random.shuffle(indices_in_chunk)\n",
    "\n",
    "            # Now subdivide this chunk's indices into mini-batches\n",
    "            for start_i in range(0, len(indices_in_chunk), self.mini_batch_size):\n",
    "                mini_batch = indices_in_chunk[start_i : start_i + self.mini_batch_size]\n",
    "                yield mini_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This is the total number of mini-batches (not the number of chunks).\n",
    "        \"\"\"\n",
    "        total_mb = 0\n",
    "        for chunk_list in self.chunks:\n",
    "            chunk_len = len(chunk_list)\n",
    "            total_mb += math.ceil(chunk_len / self.mini_batch_size)\n",
    "        return total_mb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dc275b7-d6fd-4c40-b2b2-55d5dbf721e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: 10 graphs\n",
      "Batch 1: 10 graphs\n",
      "Batch 2: 10 graphs\n",
      "Batch 3: 10 graphs\n",
      "Batch 4: 10 graphs\n",
      "Batch 5: 10 graphs\n",
      "Batch 6: 10 graphs\n",
      "Batch 7: 10 graphs\n",
      "Batch 8: 10 graphs\n",
      "Batch 9: 10 graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/dali/home/mscbio/jmc530/.conda/envs/omtra/lib/python3.11/site-packages/torch/utils/data/sampler.py:65: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def graph_collate(batch_list):\n",
    "    \"\"\"\n",
    "    batch_list is a list of (x, a, e, edge_index).\n",
    "    Return as-is or do more sophisticated collation.\n",
    "    \"\"\"\n",
    "    return batch_list\n",
    "\n",
    "dataset = ZarrDataset(root)\n",
    "\n",
    "sampler = ChunkSampler(dataset, chunk_size=10, shuffle=True)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=sampler,\n",
    "    collate_fn=graph_collate,\n",
    "    num_workers=0,  # or more if desired\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "for batch_idx, batch_graphs in enumerate(loader):\n",
    "    print(f\"Batch {batch_idx}: {len(batch_graphs)} graphs\")\n",
    "    for (x, a, e, edge_index) in batch_graphs:\n",
    "        # x.shape => (num_nodes, 3)\n",
    "        # a.shape => (num_nodes,)\n",
    "        # e.shape => (num_edges,)\n",
    "        # edge_index.shape => (num_edges, 2)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8849b60-088f-4851-826c-83ffeea823c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: Check if yeild actually reduces memory fetches\n",
    "Does having seperate stores mess this idea up?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
